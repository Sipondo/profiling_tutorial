{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adapted from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving deep learning with profiling\n",
    "=====================\n",
    "\n",
    "For this tutorial, we will train a ResNet model on the CIFAR10 dataset.\n",
    "We will record utilisation data using the inbuilt PyTorch profiling tools and change the model training process using this data.\n",
    "\n",
    "This tutorial consists of three parts:\n",
    "\n",
    "- A: Training an image classifier\n",
    "- B: Profiling our model training\n",
    "- C: Change our model training using gained insights\n",
    "\n",
    "\n",
    "The CIFAR10 dataset has the classes:\n",
    "'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse',\n",
    "'ship', 'truck'. The images in CIFAR-10 are of size 3x32x32, i.e.\n",
    "3-channel color images of 32x32 pixels in size.\n",
    "\n",
    "![cifar10](https://pytorch.org/tutorials/_static/img/cifar10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: Training an image classifier\n",
    "----------------------------\n",
    "\n",
    "1. Load dataset, ResNet and optimizer\n",
    "========================================\n",
    "\n",
    "This loads in our dataset and ResNet34 model, which are by modern standards a relatively small dataset and convolutional network, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "        'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                    download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "def define_network(batch_size=8, num_workers=1):\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=num_workers)\n",
    "\n",
    "\n",
    "    \n",
    "    net = torchvision.models.resnet34()\n",
    "\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "\n",
    "    return net, trainloader, criterion, optimizer, device\n",
    "\n",
    "net, trainloader, criterion, optimizer, device =  define_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train the network\n",
    "====================\n",
    "\n",
    "This loops over the training data and trains the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(net, trainloader, criterion, optimizer, device, epoch_count=1, stop_early=False):\n",
    "    data_length = len(trainloader)\n",
    "    for epoch in range(epoch_count):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 20 == 19:    # print every 20 mini-batches\n",
    "                print(f'Epoch {epoch + 1}, {i + 1:5d}/{data_length}] loss: {running_loss / 20:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "            if stop_early and i*trainloader.batch_size > 500: # stop early to limit the size of our profiling\n",
    "                break\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "train(net, trainloader, criterion, optimizer, device, epoch_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Test the network on the test data\n",
    "====================================\n",
    "\n",
    "To verify let us test the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks way better than chance, which is 10% accuracy (randomly\n",
    "picking a class out of 10 classes). Seems like the network learnt\n",
    "something.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B: Profiling our model training\n",
    "----------------------------\n",
    "\n",
    "Our model has trained, it took a long time to train even such a simple model. Let us collect information to figure out why training took so long.\n",
    "Increasing the efficiency of training does not only save resources; it allows for rapid debugging and training the model for more epochs in the same amount of time.\n",
    "\n",
    "There are multiple profilers available and for our basic analysis most will suffice. In order to comply with the limitations of Google Colab we can resort to the built-in PyTorch profiler and memory snapshot.\n",
    "We can wrap the train loop in a profiler context to profile model training, and we can record a GPU memory snapshot by initialising it before we start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, trainloader, criterion, optimizer, device = define_network()\n",
    "torch.cuda.memory._record_memory_history(enabled=True)\n",
    "\n",
    "with torch.profiler.profile(\n",
    "        activities=[\n",
    "            torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA,\n",
    "        ],\n",
    "        record_shapes=False,\n",
    "        profile_memory=True,\n",
    "        with_stack=False,\n",
    "    ) as prof:\n",
    "    train(net, trainloader, criterion, optimizer, device, epoch_count=1, stop_early=True)\n",
    "\n",
    "torch.cuda.memory._dump_snapshot(\"cuda_snapshot.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noteably, training our model like this should be *significantly* slower than our previous training loop. Profiling is not cheap; we are essentially logging all kinds of information which introduces substantial overhead!\n",
    "We have enabled *stop_early* to prevent this step from taking too much time; a part of one epoch will be more than enough.\n",
    "\n",
    "We have collected a lot of information. Here I welcome you to browse through the available stats yourself, but just to give a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CPU time spent on operations\")\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU/CUDA time spent on operations\")\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RAM memory allocations\")\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These provide valuable tabular data that shows us what hogs up more resources than it should, but a table only has so much expressive power.\n",
    "One way to dig deeper is by delving in the execution trace of our model training. This gives us a timeline based view that allows us to find bottlenecks effectively.\n",
    "\n",
    "We can dump a trace using the PyTorch profiler. Google Chrome can open trace files natively, but the interface is rather minimal. I recommend opening your trace file in Perfetto instead: [https://ui.perfetto.dev/#!/viewer](https://ui.perfetto.dev/#!/viewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.export_chrome_trace(\"trace.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we can delve into the CUDA memory snapshot that we have saved. This allows us to figure out how our model training utilised the GPU memory.\n",
    "Go to [https://pytorch.org/memory_viz](https://pytorch.org/memory_viz) to analyze the memory snapshot `.pickle` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C: Change our model training using gained insights\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these sources of information, we can drastically speed up the execution of our model training. The training can be tweaked in multiple ways, and I wholeheartedly recommend you to experiment and see the effects on the profiling.\n",
    "In particular I would like to bring your attention to two aspects:\n",
    "\n",
    "Batch Size\n",
    "----------\n",
    "\n",
    "Inspect the GPU memory utilisation. Is the GPU memory saturated or is there still much to work with? Additionally, is the GPU working constantly, on full power, or is it idling for a decent chunk of time?\n",
    "GPU's are efficient for deep learning training due to their ability to run massively parallel operations efficiently, and increasing the degree of parallellism in our model training can be highly beneficial to training speed.\n",
    "Right now the `batch size` is set to `8`, which means that the model trains on 8 images at the same time. Try increasing this to `64` or even `128`, what is the observed effect on training speed and profiling?\n",
    "\n",
    "Dataloading Worker Count\n",
    "----------\n",
    "\n",
    "Inspect the trace plot. Are the CPU and GPU overlapping operations? They should be able to run operations at the same time as long as there are no hard dependencies from forcing one to wait on the other. When using a basic data loader the data is being read from disk and processed whenever the GPU is done with processing the previous data. We can fetch this data while the GPU is working to prevent this stalling from happening. This can be done by adjusting `num_workers` to a higher number than `1`, e.g. `4`. What effect can you observe on the training speed and the trace?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
