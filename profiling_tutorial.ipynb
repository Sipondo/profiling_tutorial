{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adapted from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Started\n",
    "---------------\n",
    "\n",
    "If you are running this in Google Colab, a few points to get started.\n",
    "You may want to start off by saving this as a copy to your Drive. This allows you to edit this notebook.\n",
    "\n",
    "![copy to drive](img/drivecopy.png)\n",
    "\n",
    "Furthermore, this notebook requires a GPU to run properly. You can change your runtime to use a GPU in the topright corner.\n",
    "\n",
    "![change runtime](img/changeruntime.png)\n",
    "\n",
    "Once clicked a window will open that allows you to specify whether you want to use a GPU-enabled node.\n",
    "\n",
    "![select gpu](img/runtimetype.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run CLI commands using the `!` prefix. Let's run `nvidia-smi` to figure out what GPU we have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see one GPU listed (e.g. Tesla T4). If you don't see a GPU listed, or the command was not found, please swap your runtime to a GPU-enabled one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving deep learning with profiling\n",
    "=====================\n",
    "\n",
    "For this tutorial, we will train a ResNet model on the CIFAR10 dataset.\n",
    "We will record utilisation data using the inbuilt PyTorch profiling tools and change the model training process using this data.\n",
    "\n",
    "This tutorial consists of three parts:\n",
    "\n",
    "- A: Training an image classifier\n",
    "- B: Profiling our model training\n",
    "- C: Change our model training using gained insights\n",
    "\n",
    "\n",
    "The CIFAR10 dataset has the classes:\n",
    "'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse',\n",
    "'ship', 'truck'. The images in CIFAR-10 are of size 3x32x32, i.e.\n",
    "3-channel color images of 32x32 pixels in size.\n",
    "\n",
    "![cifar10](https://pytorch.org/tutorials/_static/img/cifar10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: Training an image classifier\n",
    "----------------------------\n",
    "\n",
    "1. Load dataset, ResNet and optimizer\n",
    "========================================\n",
    "\n",
    "This loads in our dataset and ResNet34 model, which are by modern standards a relatively small dataset and convolutional network, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "        'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                    download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "def define_network(batch_size=8, num_workers=1):\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=num_workers)\n",
    "\n",
    "\n",
    "    \n",
    "    net = torchvision.models.resnet34()\n",
    "\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "\n",
    "    return net, trainloader, criterion, optimizer, device\n",
    "\n",
    "net, trainloader, criterion, optimizer, device =  define_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train the network\n",
    "====================\n",
    "\n",
    "This loops over the training data and trains the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(net, trainloader, criterion, optimizer, device, epoch_count=1, stop_early=False):\n",
    "    data_length = len(trainloader)\n",
    "    for epoch in range(epoch_count):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 20 == 19:    # print every 20 mini-batches\n",
    "                print(f'Epoch {epoch + 1}, {i + 1:5d}/{data_length}] loss: {running_loss / 20:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "            if stop_early and i*trainloader.batch_size > 500: # stop early to limit the size of our profiling\n",
    "                break\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "train(net, trainloader, criterion, optimizer, device, epoch_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Test the network on the test data\n",
    "====================================\n",
    "\n",
    "To verify let us test the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks way better than chance, which is 10% accuracy (randomly\n",
    "picking a class out of 10 classes). Seems like the network learnt\n",
    "something.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B: Profiling our model training\n",
    "----------------------------\n",
    "\n",
    "Our model has trained, it took a long time to train even such a simple model. Let us collect information to figure out why training took so long.\n",
    "Increasing the efficiency of training does not only save resources; it allows for rapid debugging and training the model for more epochs in the same amount of time.\n",
    "\n",
    "There are multiple profilers available and for our basic analysis most will suffice. In order to comply with the limitations of Google Colab we resort to the built-in PyTorch profiler.\n",
    "We can wrap the train loop in a profiler context to profile model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, trainloader, criterion, optimizer, device = define_network()\n",
    "with torch.profiler.profile(\n",
    "        activities=[\n",
    "            torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA,\n",
    "        ],\n",
    "        record_shapes=False,\n",
    "        profile_memory=True,\n",
    "        with_stack=False,\n",
    "    ) as prof:\n",
    "    train(net, trainloader, criterion, optimizer, device, epoch_count=1, stop_early=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noteably, training our model like this should be *significantly* slower than our previous training loop. Profiling is not cheap; we are essentially logging all kinds of information which introduces substantial overhead!\n",
    "We have enabled *stop_early* to prevent this step from taking too much time; a part of one epoch will be more than enough.\n",
    "\n",
    "We have collected a lot of information. Here I welcome you to browse through the available stats yourself, but just to give a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CPU time spent on operations\")\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU/CUDA time spent on operations\")\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RAM memory allocations\")\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These provide valuable tabular data that shows us what hogs up more resources than it should, but a table only has so much expressive power.\n",
    "One way to dig deeper is by delving in the execution trace of our model training. This gives us a timeline based view that allows us to find bottlenecks effectively.\n",
    "\n",
    "We can dump a trace using the PyTorch profiler. Google Chrome can open trace files natively [about:tracing](about:tracing), but for other browsers you can use Perfetto instead [https://ui.perfetto.dev/#!/viewer](https://ui.perfetto.dev/#!/viewer).\n",
    "\n",
    "Files that you save in Colab are found in the file tab to the left. You may have to press the refresh arrow in the file browser. You can download files using the right mouse button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.export_chrome_trace(\"trace.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can dive into how our program uses the GPU memory using a PyTorch CUDA memory snapshot. For this we do want to train the model for at least a full epoch in order to get a good picture of our memory consumption. We can record a GPU memory snapshot by initialising it before we start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory._record_memory_history(max_entries=100000)\n",
    "\n",
    "net, trainloader, criterion, optimizer, device = define_network()\n",
    "\n",
    "train(net, trainloader, criterion, optimizer, device, epoch_count=1, stop_early=False)\n",
    "\n",
    "torch.cuda.memory._dump_snapshot(\"cuda_snapshot.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to [https://pytorch.org/memory_viz](https://pytorch.org/memory_viz) to analyze the memory snapshot `.pickle` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C: Change our model training using gained insights\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these sources of information, we can drastically speed up the execution of our model training. The training can be tweaked in multiple ways, and I wholeheartedly recommend you to experiment and see the effects on the profiling. Remember that the effectiveness heavily depends on what hardware you are running on; some things may work extremely well while others might not benefit training at all.\n",
    "\n",
    "As an example I would like to bring your attention to two aspects:\n",
    "\n",
    "Batch Size\n",
    "----------\n",
    "\n",
    "Is the GPU memory saturated or is there still much to work with? Additionally, is the GPU working constantly, on full power, or is it idling for a decent chunk of time?\n",
    "GPU's are efficient for deep learning training due to their ability to run massively parallel operations efficiently, and increasing the degree of parallellism in our model training can be highly beneficial to training speed.\n",
    "Right now the `batch size` is set to `8`, which means that the model trains on 8 images at the same time. Try increasing this to `64` or even `128`, what is the observed effect on training speed and profiling?\n",
    "\n",
    "Dataloading Worker Count\n",
    "----------\n",
    "\n",
    "Inspect the trace plot. Are we spending all of our time training on our data, or do we need to wait for our data? When using a basic data loader the data is being read from disk and processed whenever the GPU is done with processing the previous data. We can fetch this data while the GPU is working to prevent this stalling from happening. This can be done by adjusting `num_workers` to a higher number than `1`, e.g. `16`. What effect do you observe on the training speed and the trace?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
